{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb project - Lennart & Thomas\n",
    "\n",
    "(If you want to run the code included in the notebook below, please make sure that you have downloaded the proper .pickle files and put them in the same directory as your project.)\n",
    "\n",
    "# Motivation\n",
    "## What is your dataset?\n",
    "Our dataset is mainly based on a dataset from [kaggle.com](https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset), which contains ~5000 movies with attributes such as title, director, duration, budget, gross and more. However this dataset is not fulfilling in respect to cast members of each movie, so we installed a Python package called [IMDbPY](http://imdbpy.sourceforge.net/). IMDbPY lets us input a movie title, query the IMDb online database and gain access to a lot of extra attributes - including a satisfying cast list for each movie. \n",
    "\n",
    "Other than a Kaggle set and IMDbPY, we have also download a bunch of manuscripts, for text analysis, from imsdb.com and springfieldspringfield.co.uk. In total we managed to parse about 1200 scripts from the websites.\n",
    "\n",
    "## Why did you choose this/these particular dataset(s)?\n",
    "Our initial idea, as we mentioned in our Project Assignment A video, was to gather all the movie information from Wikipedia in similar style to the Philosopher Network. We had downloaded quite a bit of data before we ran into trouble with getting cast, budget, gross and other data that might not be listed in the same way for each WikiPage.\n",
    "\n",
    "Then we stubled upon the dataset from Kaggle. It had all the information we needed for the project and was available in clean, well almost, clean .csv files. We then found IMDbPY and it was a perfect fit for getting even more attributes for each movie.\n",
    "\n",
    "For manuscripts imsdb.com and springfieldspringfield.co.uk had the most comprehensive databse of manuscripts. Unfortunately they are not offering the scripts as download, so here we used our skills from the class to parse the HTML using BeautifulSoup and save the manuscripts locally using Pickle. \n",
    "\n",
    "## What was your goal for the end user's experience?\n",
    "Our end goal is to present the user with some information:\n",
    "* Regarding the different aspects of making a successful movie by looking at different parameters that makes a successful movie.\n",
    "* On how characteristics of how movies are connected by their actors and to discover patterns in this network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic stats. Let's understand the dataset better\n",
    "## Write about your choices in data cleaning and preprocessing\n",
    "### Movie data\n",
    "We chose to download a dataset from kaggle which was ordered in different columns contained in a csv file. We used the panda library to read the csv files and it provides an easy use of the information contained in the csv file. The dataset was missing meaningful information about the cast of each movie, as it had only three actors for each movie, and sometimes the main characters of the movie was not there. So to obtain this information we used the IMDbPy to request the cast for each movie and then replaced the actors originally in the dataset with this set of actors. Combining the two in a dict we had all the information needed to begin analyzing the movies.\n",
    "To remove some unicode problems we had to replace two special unicode characters from each title using (.replace) with the empty string, as every title ended with this. This was necessary as we had to combine two datasets and the movies titles would not match otherwise.\n",
    "\n",
    "### Scripts\n",
    "We used the BeautifulSoup library to download and parse text from an html source of the websites. This allows for us to search for a specific keyword/tag and download everything contained in this tag. So we checked to source code for the html pages we wanted to download the scripts from and found where the text was located and could then download it. \n",
    "\n",
    "Afterwards we did some cleanup of the scripts. We converted each script to a string for which we first removed all tags using a regular expression and then deleted newline symbols, question marks, commas etc.  \n",
    "As we wanted to analyze only the words from the scripts we used another regular expression to remove anything not being a word. Then we tokenized the text using nltks regular expression tokenizer with the regular expression removing anything but words. \n",
    "We imported stopwords and added a few extra ourselves and then for each token we converted it to lowercase and if not a stopword and more than two letters we added it. We chose to remove words at length less than three as many of them were not real words but a result of the tokenizer splitting the words incorrectly. \n",
    "\n",
    "### Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)\n",
    "\n",
    "As previously mentioned we used a dataset with a lot of attributes. At first glance it seemed a perfect fit for our project, but as we started analysing the data we found some problems with the set. For example the gross of each movie, which is just labeled 'gross' in the csv file, is the domestic gross of the movie, not the international. The budget numbers were faulty as well .. the information describing the set specified that the budgets were declared in american dollars, USD. However some movies, for example from South Korea, were specified in Korean currency which meant that it looked like a couple of movies costed \\$4 billion to produce which is so far from the average that it didn't make sense.\n",
    "\n",
    "We mentioned before that we weren't satisfied with the cast members of the set. There is only 3 actors per movie in the set and those in the set were included based on their facebook likes, which did leave some prominent actors out. So to work around this we used IMDbPY as explained before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory. Which theoretical tools did you use?\n",
    "\n",
    "## Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "\n",
    "We mostly used community detection on our network of movies. This way we could tell if certain genres shared the same actors, but we will explain this further in the section 'Tools and analysis' below.\n",
    "\n",
    "## Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "\n",
    "We mostly used regular expressions to clean the data we had downloaded by removing tags in the scripts which were being downloaded as plain HTML pages. We didn't use unicode that much. By using the kaggle set we got mostly clean data, so there was no need.\n",
    "\n",
    "## Tools and analysis:\n",
    "\n",
    "## Graph representation & Genre Communities\n",
    "**The idea:**\n",
    "\n",
    "As the movies form a network we want to see how they are connected inside this network by visualizing it. Specifically we want to see how communities in this network look like, and also see if it has any relation to the movie genres. We want to compare the movie genres with the communities and see whether the movie genres are communities. \n",
    "\n",
    "**The tool**\n",
    "\n",
    "We use the Louvain algorithm for community detection in a graph. This automatically finds the best partition of the graph so that each partition form a community. We use the *max* function of *nx.connected-component-subgraphs* which finds the greatest connected component of graph. We iterate through the nodes in each community and count occurrences of each genre in each community to see if the genres represent communities. \n",
    "\n",
    "**The python code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Movies connected by actors\n",
    "#G = nx.Graph()\n",
    "#for title, attr in films_dict.iteritems():\n",
    "#    G.add_node(title)\n",
    "#    for actor1 in attr['cast']:        \n",
    "#        for title2, attr2 in films_dict.iteritems():\n",
    "#            if actor1 in attr2['cast'] and title != title2: \n",
    "#                G.add_edge(title, title2)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('films_graph.pickle', 'rb') as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.draw(G,node_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eig = nx.eigenvector_centrality(G)\n",
    "sorted_eig = sorted(eig, key = eig.get,reverse = True)\n",
    "\n",
    "for i in range(0,10):\n",
    "    p = sorted_eig[i]\n",
    "    print p,eig[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sorted_betweenness_cent_movies_graph.pickle', 'rb') as f:\n",
    "    sorted_btc = cPickle.load(f)\n",
    "    \n",
    "for i in range(0,10):    \n",
    "    print sorted_btc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "giant = max(nx.connected_component_subgraphs(G), key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.draw(giant, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partition = community.best_partition(giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part = community.best_partition(giant)\n",
    "mod = community.modularity(part,giant)\n",
    "\n",
    "values = [part.get(node) for node in giant.nodes()]\n",
    "\n",
    "fig_size = [18,14]\n",
    "\n",
    "nx.draw_spring(giant, scale=3,cmap=plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False, linewidth = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Iterate through communities and count occurences of each genre in each community.\n",
    "com_gen_dict = {}\n",
    "gen_count = {}\n",
    "for com in set(partition.values()) :\n",
    "    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "    gen_count = {}\n",
    "    for n in list_nodes:\n",
    "        for title, attr in films_dict.iteritems():        \n",
    "            if n == title:\n",
    "                genre = attr['genres'][0]\n",
    "                genre = str(genre)\n",
    "                if gen_count.get(genre) is None:\n",
    "                    gen_count[genre] = 1\n",
    "                else:\n",
    "                    gen_count[genre] += 1 \n",
    "    com_gen_dict[com] = gen_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a list of all unique genres\n",
    "all_gens = []\n",
    "for k,v in com_gen_dict.iteritems():\n",
    "    for g in v.keys():\n",
    "        all_gens.append(g)\n",
    "\n",
    "        \n",
    "all_gens = set(all_gens)\n",
    "all_gens = list(all_gens)\n",
    "all_gens = sorted(all_gens)\n",
    "\n",
    "print len(all_gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "ods = []\n",
    "for k,v in com_gen_dict.iteritems():\n",
    "    od = collections.OrderedDict(sorted(v.items()))\n",
    "    ods.append(od.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 11.0\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(14,14))\n",
    "dd = pd.DataFrame(ods[0],columns=[''])\n",
    "dd.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[0,0],legend=False)\n",
    "\n",
    "dd2 = pd.DataFrame(ods[1],columns=[''])\n",
    "dd2.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[0,1],legend=False)\n",
    "\n",
    "dd3 = pd.DataFrame(ods[2],columns=[''])\n",
    "dd3.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[0,2],legend=False)\n",
    "\n",
    "dd3 = pd.DataFrame(ods[3],columns=[''])\n",
    "dd3.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[1,0],legend=False)\n",
    "\n",
    "dd2 = pd.DataFrame(ods[4],columns=[''])\n",
    "dd2.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[1,1],legend=False)\n",
    "\n",
    "dd3 = pd.DataFrame(ods[5],columns=[''])\n",
    "dd3.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[1,2],legend=False)\n",
    "\n",
    "dd3 = pd.DataFrame(ods[6],columns=[''])\n",
    "dd3.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[2,0],legend=False)\n",
    "\n",
    "dd2 = pd.DataFrame(ods[7],columns=[''])\n",
    "dd2.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[2,1],legend=False)\n",
    "\n",
    "dd3 = pd.DataFrame(ods[8],columns=[''])\n",
    "dd3.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[2,2],legend=False)\n",
    "\n",
    "dd3 = pd.DataFrame(ods[9],columns=[''])\n",
    "dd3.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[3,0],legend=False)\n",
    "\n",
    "dd2 = pd.DataFrame(ods[10],columns=[''])\n",
    "dd2.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[3,1],legend=False)\n",
    "\n",
    "dd3 = pd.DataFrame(ods[11],columns=[''])\n",
    "dd3.plot.pie(subplots=True, colormap='Paired', labels=all_gens,ax=axes[3,2],legend=False)\n",
    "\n",
    "plt.tight_layout(pad=4.4,w_pad=10.5,h_pad=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The outcome**\n",
    "\n",
    "The result of this is plotted as pie charts and from this plot we clearly see that the genres are not really communities. The communities consist of many different genres and only one of the communities has a clear overweight of one genre, namely the one community which has more than 3/4 movies being documentary movies. So while we can't say that the genres are communities there are some specific genres, which differentiate a lot from other movie genres, that could be communities. Most actors fall in the category of being in a lot of different movie genres and some are primarily in e.g. documentaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genres & words\n",
    "**The idea:**\n",
    "\n",
    "We want to see what the characteristic words for a movie and a genre of movies are. We would like to see whether this corresponds to what we would expect from a movie genre. \n",
    "\n",
    "**The tool**\n",
    "\n",
    "For this we calculate TF-IDF which is the term frequency - inverse document frequency. This is the amount of occurrences of a word in one text multiplied with the inverse document frequency which gives lower scores to words frequently occurring in other texts as well. Thus the tf-idf is in indicator as to which words actually say something about a specific text giving high scores to the words that differentiates it from other texts. \n",
    "From these tf-idfs we make wordclouds which automatically correlates the tf-idf score of a word with the size of the word in the wordcloud.\n",
    "\n",
    "**The Python code**\n",
    "\n",
    "*Include Python Code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import io\n",
    "import nltk\n",
    "import json\n",
    "import unicodedata\n",
    "import math\n",
    "import urllib\n",
    "import urllib2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cPickle\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "sw = stopwords.words('english')\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "%matplotlib inline\n",
    "fig_size = [14,6]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r'\\s*\\<[\\w\\s\\=\\\"\\-\\/]*\\>', '', text) # All tags - i.e. <br/> <div> etc.\n",
    "    text = text.replace('@','')\n",
    "    text = text.replace('!','')\n",
    "    text = text.replace('.','')\n",
    "    text = text.replace(',','')\n",
    "    text = text.replace('?','')\n",
    "    text = text.replace('-','')\n",
    "    text = text.replace('\\n','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('script_dict.pickle', 'rb') as fp:\n",
    "       script_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_tokens = {}\n",
    "for title, script in script_dict.iteritems():\n",
    "    text = str(script)\n",
    "    text2 = clean(text)\n",
    "\n",
    "    regex = r'\\s*[^A-Za-z]+([a-z]+)'\n",
    "    tokens = nltk.regexp_tokenize(text2, regex)\n",
    "    \n",
    "    sw2 = sw + [\"you're\", \"don't\", \"i'm\", \"l'm\", \"i'll\", \"should've\", \"\"] # adding extra stopwords\n",
    "    tokens = [t.lower() for t in tokens if t.lower() not in sw2 and len(t) > 2]\n",
    "\n",
    "    text_tokens[title] = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for t, v in text_tokens.items():\n",
    "    for tt in v:\n",
    "        all_words.append(tt)\n",
    "\n",
    "all_words = set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfs_map = {}\n",
    "for key, val in text_tokens.items():\n",
    "    fd = nltk.FreqDist(val)\n",
    "    tfs_map[key] = fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idfs = {}\n",
    "for t in all_words:\n",
    "    occ = 0\n",
    "    for w in script_dict.keys():\n",
    "        tokens = text_tokens[w]\n",
    "        if t in tokens:\n",
    "            occ += 1\n",
    "\n",
    "    idfs[t] = math.log(len(script_dict.keys())/float(occ+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfds = {}\n",
    "for title, wrds in tfs_map.iteritems():\n",
    "    tfds[title] = {}\n",
    "    for word in wrds:\n",
    "        tfds[title][word] = tfs_map[title][word] * idfs[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "films = {}\n",
    "for k, array in tfds.items():\n",
    "    \n",
    "    one_string = ''\n",
    "    for (word,score) in array.items():\n",
    "        for ii in range(0,int(math.ceil(score))):\n",
    "            one_string += word + ' '\n",
    "            \n",
    "    films[k] = one_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('films_dict.pickle','rb') as f:\n",
    "    films_dict2 = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres = {}\n",
    "for title in films.keys():\n",
    "    genre = films_dict2[title]['genres'][0]\n",
    "    genres[genre] = []\n",
    "        \n",
    "for genre, movies in genres.iteritems():\n",
    "    for title in films.keys():\n",
    "        if genre == films_dict2[title]['genres'][0]:\n",
    "            movies.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['Romance','Musical','Horror','Biography']\n",
    "\n",
    "for ii in  range(0,2):\n",
    "    fig = plt.figure(figsize=(18,10))\n",
    "    genre = names[ii*2]\n",
    "    movies = genres[genre]\n",
    "    string = ''\n",
    "    for m in movies:\n",
    "        try:\n",
    "            string += films[m]\n",
    "        except:\n",
    "            continue        \n",
    "\n",
    "    wordcloud = WordCloud(background_color = 'black',\n",
    "                          width=1400,\n",
    "                          height=1000).generate(string)\n",
    "\n",
    "    plt.subplot2grid((3,3),(0,0), rowspan = 2)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(genre,fontsize=18)\n",
    "    plt.axis('off')\n",
    "\n",
    "    genre = names[ii + 1 + 1*ii]\n",
    "    movies = genres[genre]\n",
    "    string = ''\n",
    "    for m in movies:\n",
    "        try:\n",
    "            string += films[m]\n",
    "        except:\n",
    "            continue        \n",
    "\n",
    "    wordcloud = WordCloud(background_color = 'black',\n",
    "                          width=1400,\n",
    "                          height=1000).generate(string)\n",
    "\n",
    "    plt.subplot2grid((3,3),(0,1), rowspan = 2)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(genre,fontsize=18)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome**\n",
    "\n",
    "For the genres we find that the wordclouds are actually very descriptive and emphasizes some of the words we ourselves would associate with a movie genre. E.g. For the musical genre words like sing, artist and beloved are some of the keywords - being the words with the highest tf-idf. Romance has flowers, Christmas, raspberries which you can easily associate with a romantic movie. The tf-idf is therefore quite informative and can give an indication as to whether the genre would be interesting for you to watch as you can check the keywords. But you can also use it for just one movie and thereby get a good understanding of the plot of the movie.\n",
    "\n",
    "\n",
    "## Sentiment\n",
    "**The idea:**\n",
    "\n",
    "We want to find the happiness/negativeness of all the manuscripts we have downloaded. These sentiment scores will be stored in a dictionary with the name of the movies the keys and the scores as the values. We can then cross reference the titles to our dataset and get some insight on possible relations. We want to see if there is a correlation between a movie's sentiment in the script and the movie's IMDb rating, and do the same for actors and their sentiment.\n",
    "\n",
    "**The tool**\n",
    "\n",
    "The tool is the calculating the sentiment based on the LabMT wordlist(include reference). The LabMT wordlist is a list of 10,000 evaluated words in regards of happiness, where the 10,000 words are chosen solely on usage frequency.\n",
    "So for each script we clean it by removing HTML tags, then we split it up in tokens and finally accumulate the total sentiment score.\n",
    "\n",
    "**The Python code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = io.open('./labmt.txt', 'r', encoding='utf-8')\n",
    "\n",
    "labmt = {}\n",
    "\n",
    "titles = nltk.word_tokenize(f.readline())# To get the titles out\n",
    "\n",
    "for line in f:\n",
    "    #tokens = nltk.word_tokenize(line)\n",
    "    tokens = line.split('\\t')\n",
    "    \n",
    "    scores = {}\n",
    "    scores[titles[2]] = tokens[2]\n",
    "\n",
    "    labmt[tokens[0]] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeTags(text):\n",
    "    text = re.sub(r'\\s*\\<[\\w\\s\\=\\\"\\-\\/]*\\>', '', text) # All tags - i.e. <br/> <div> etc.\n",
    "    return text\n",
    "\n",
    "def sentiment_score(text):\n",
    "    result = 0.0\n",
    "    text = removeTags(text)\n",
    "    tokens = text.split(' ')\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    for w in tokens:\n",
    "        if w in labmt.keys() and w.isalpha():\n",
    "            data = labmt[w]['happiness_average']\n",
    "            if data != '--' and data != 't':\n",
    "                result += float(data)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_rating = []\n",
    "for k,v in films_dict2.iteritems():\n",
    "    movie_rating.append( (k,v['rating']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_rating = sorted(movie_rating, key=lambda tup: tup[1], reverse=True)\n",
    "top_scripts = []\n",
    "for title, script in script_dict.iteritems():\n",
    "    for (t,r) in sorted_rating:\n",
    "        if title == t:\n",
    "            top_scripts.append( (title,r) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (t,r) in top_scripts:\n",
    "    if t not in senti_script.keys():\n",
    "        senti_score = sentiment_score(script_dict.get(t))\n",
    "        senti_script[t] = {'score': senti_score, 'rating': r}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate sentiment vs. IMDb rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "ratings = []\n",
    "\n",
    "for k,v in senti_script.iteritems():\n",
    "    scores.append(v['score'])\n",
    "    ratings.append(v['rating'])\n",
    "    \n",
    "plt.scatter(scores,ratings)\n",
    "plt.ylim(0,10)\n",
    "plt.title('Movie rating vs. its script sentiment', fontsize=18)\n",
    "plt.ylabel('IMDb rating', fontsize=14)\n",
    "plt.xlabel('Total sentiment', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment for actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actor_movies = defaultdict(list)\n",
    "for title, attr in films_dict2.iteritems():\n",
    "    for t in senti_script.keys():\n",
    "        if title == t:\n",
    "            for a in attr['cast']:\n",
    "                actor_movies[a].append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actor_senti = defaultdict(list)\n",
    "for a, mvs in actor_movies.iteritems():\n",
    "    for t, vls in senti_script.iteritems():\n",
    "        if t in mvs:\n",
    "            actor_senti[a].append(vls['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_actor_senti = defaultdict(list)\n",
    "for a, r in actor_senti.iteritems():\n",
    "    if len(r) > 5:\n",
    "        average_actor_senti[a] = np.average(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottm10 = sorted(average_actor_senti.items(), key=operator.itemgetter(1))[:10]\n",
    "top10 = sorted(average_actor_senti.items(), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "# Top\n",
    "names = [n for (n,v) in top10]\n",
    "rating = [v/1000 for (n,v) in top10]\n",
    "acts = np.arange(len(names))\n",
    "acts = sorted(acts, reverse=True)\n",
    "\n",
    "\n",
    "plt.subplot2grid((2,3),(0,0), rowspan = 2)\n",
    "plt.barh(acts, rating, align='center', color='g')\n",
    "plt.yticks(acts, names)\n",
    "plt.ylim(-1,len(names))\n",
    "plt.title(\"Top 10 actors in > 5 movies - sentiment\", fontsize = 18)\n",
    "plt.ylabel(\"Actors\", fontsize = 14)\n",
    "plt.xlabel(\"Average sentiment (in 1000's)\", fontsize=14)\n",
    "\n",
    "# Bottom\n",
    "names2 = [n for (n,v) in bottm10]\n",
    "rating2 = [v for (n,v) in bottm10]\n",
    "acts2 = np.arange(len(names2))\n",
    "acts2 = sorted(acts2, reverse=True)\n",
    "\n",
    "plt.subplot2grid((2,3),(0,1), rowspan = 2)\n",
    "plt.barh(acts2, rating2, align='center', color='r')\n",
    "plt.yticks(acts2, names2)\n",
    "plt.ylim(-1,len(names))\n",
    "plt.title(\"Bottom 10 actors in > 5 movies - sentiment\", fontsize = 18)\n",
    "plt.xlabel(\"Average sentiment\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate actor sentiment with IMDb rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actor_scores = defaultdict(list)\n",
    "for title, attr in films_dict2.iteritems():\n",
    "    for a in attr['cast']:\n",
    "        actor_scores[a].append(attr['imdb_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actor_sent_score = {}\n",
    "for a in actor_senti.keys():\n",
    "    actor_sent_score[a] = {'rating': actor_scores.get(a), 'senti': actor_senti.get(a)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "ratings = []\n",
    "for k, v in actor_sent_score.iteritems():\n",
    "    if len(v['senti']) > 5: \n",
    "        sents.append(np.average(v['senti']))\n",
    "        ratings.append(np.average(v['rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(sents,ratings)\n",
    "plt.ylim(0,10)\n",
    "plt.title('Sentiment of actors vs. their IMDb Rating', fontsize = 18)\n",
    "plt.ylabel('IMDb Rating', fontsize = 14)\n",
    "plt.xlabel('Sentiment of actor', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome**\n",
    "\n",
    "The result is a few plots to show relation between scripts' sentiment and IMDb for movies and actors respectivley.\n",
    "Unfortunately it doesn't uncover some surprising relations, but more whats expected - that the happiness doesn't necessarily determine the IMDb rating of movies or actors.\n",
    "\n",
    "## General Data & Correlations\n",
    "**The idea:**\n",
    "The idea is that we want to look at different attributes from our data set and present them to the users. For example we want to list the 10 best and 10 worst actors, as it might provide the users with some new knowledge.\n",
    "\n",
    "We also want to set different attributes up against each other to see if there are some sort of patterns that the user might find interesting. \n",
    "\n",
    "**The tool**\n",
    "General python structures, mostly dictionaries and lists. Dictionaries are preferred over lists in this case as the running time will be increased significantly if lists are used.\n",
    "\n",
    "**Python Code**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actors and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_actor_rating = {}\n",
    "for (a,r) in actor_scores.items():\n",
    "    if len(r) > 10:\n",
    "        average_actor_rating[a] = np.average(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottm10 = sorted(average_actor_rating.items(), key=operator.itemgetter(1))[:10]\n",
    "top10 = sorted(average_actor_rating.items(), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "# Top\n",
    "names = [n for (n,v) in top10]\n",
    "rating = [v for (n,v) in top10]\n",
    "acts = np.arange(len(names))\n",
    "acts = sorted(acts, reverse=True)\n",
    "\n",
    "plt.subplot2grid((2,3),(0,0), rowspan = 2)\n",
    "plt.barh(acts, rating, align='center', color='g')\n",
    "plt.xlim(0,10)\n",
    "plt.yticks(acts, names)\n",
    "plt.title(\"Top 10 actors in more than 10 movies - based on IMDB rating\")\n",
    "plt.ylabel(\"Actors\")\n",
    "plt.xlabel(\"Average IMDB score\", fontsize=14)\n",
    "\n",
    "# Bottom\n",
    "names2 = [n for (n,v) in bottm10]\n",
    "rating2 = [v for (n,v) in bottm10]\n",
    "acts2 = np.arange(len(names2))\n",
    "acts2 = sorted(acts2, reverse=True)\n",
    "\n",
    "plt.subplot2grid((2,3),(0,1), rowspan = 2)\n",
    "plt.barh(acts2, rating2, align='center', color='r')\n",
    "plt.xlim(0,10)\n",
    "plt.yticks(acts2, names2)\n",
    "plt.title(\"Bottom 10 actors in more than 10 movies - based on IMDB rating\")\n",
    "plt.xlabel(\"Average IMDB score\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genres and ratings/gross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('film_money_dict.pickle','rb') as f:\n",
    "    films_dict = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_genres = []\n",
    "for title, attr in films_dict.iteritems():\n",
    "    for g in attr['genres']:\n",
    "        all_genres.append(g)\n",
    "        \n",
    "all_genres = set(all_genres)\n",
    "all_genres = list(all_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre_scores = defaultdict(list)\n",
    "for title, attr in films_dict.iteritems():\n",
    "    for g in all_genres:\n",
    "        if g in attr['genres']:\n",
    "            genre_scores[g].append(attr['imdb_score'])\n",
    "\n",
    "average_genre_rating = {}\n",
    "for (g,r) in genre_scores.items():\n",
    "    average_genre_rating[g] = np.average(r)\n",
    "    \n",
    "sorted_genres = sorted(average_genre_rating.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genre_gross = defaultdict(list)\n",
    "for title, attr in films_dict.iteritems():\n",
    "    for g in all_genres:\n",
    "        if g in attr['genres']:\n",
    "            gross = attr['gross']\n",
    "            if str(gross) != 'nan':\n",
    "                genre_gross[g].append(float(attr['gross']))\n",
    "\n",
    "average_gross_rating = {}\n",
    "for (g,r) in genre_gross.items():\n",
    "    average_gross_rating[g] = np.average(r)\n",
    "    \n",
    "sorted_gross = sorted(average_gross_rating.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "names = [n for (n,v) in sorted_genres]\n",
    "rating = [v for (n,v) in sorted_genres]\n",
    "acts = np.arange(len(names))\n",
    "acts = sorted(acts, reverse=True)\n",
    "\n",
    "plt.subplot2grid((1,2),(0,0), rowspan = 2)\n",
    "plt.barh(acts, rating, height=0.9, linewidth=0.1, align='center', color='g')\n",
    "plt.xlim(0,10)\n",
    "plt.ylim(-1, len(names))\n",
    "plt.yticks(acts, names)\n",
    "plt.title(\"Best genres based on IMDb rating\", fontsize=20)\n",
    "plt.xlabel(\"Average IMDB score\", fontsize=16)\n",
    "\n",
    "\n",
    "names = [n for (n,v) in sorted_gross]\n",
    "gross = [(v/1000000) for (n,v) in sorted_gross] # Reduce gross number\n",
    "acts = np.arange(len(names))\n",
    "acts = sorted(acts, reverse=True)\n",
    "\n",
    "plt.subplot2grid((1,2),(0,1), rowspan = 2)\n",
    "plt.barh(acts, gross, height=0.9, linewidth=0.1, align='center', color='g')\n",
    "plt.ylim(-1, len(names))\n",
    "plt.yticks(acts, names)\n",
    "plt.title(\"Best genres based on gross\", fontsize=20)\n",
    "plt.xlabel(\"Average gross (in mil. $)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = []\n",
    "gross = [] \n",
    "sort_g_r = []\n",
    "for n,v in sorted_genres:\n",
    "    for n2,v2 in sorted_gross:\n",
    "        if n == n2:\n",
    "            sort_g_r.append((v,(v2/1000000)))\n",
    "            \n",
    "sort_g_r = sorted(sort_g_r, key=lambda x: x[0],reverse=True)\n",
    "ratings = [n for (n,v) in sort_g_r]\n",
    "gross = [v for (n,v) in sort_g_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(gross,ratings)\n",
    "axes = plt.gca()\n",
    "m, b = np.polyfit(gross, ratings, 1)\n",
    "X_plot = np.linspace(axes.get_xlim()[0],axes.get_xlim()[1],100)\n",
    "plt.plot(X_plot, m*X_plot + b, '-')\n",
    "plt.xlabel('Gross (in mil. $)', fontsize=16)\n",
    "plt.ylabel('IMDb Rating', fontsize=16)\n",
    "plt.title('Genres average IMDb Rating vs. average Gross', fontsize=18)\n",
    "plt.xlim(-1,max(gross)+10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_dirs = []\n",
    "\n",
    "for t, attr in films_dict.iteritems():\n",
    "    if str(attr['director']) != 'nan':\n",
    "        all_dirs.append(attr['director'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_rating = defaultdict(list)\n",
    "for d in all_dirs:\n",
    "    for t, attr in films_dict.iteritems():\n",
    "        if d == attr['director']:\n",
    "            dir_rating[d].append(attr['imdb_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_dir_rating = defaultdict(list)\n",
    "for k,v in dir_rating.iteritems():\n",
    "    if len(v) > 5:\n",
    "        avg_dir_rating[k] = np.average(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottm10 = sorted(avg_dir_rating.items(), key=operator.itemgetter(1))[:10]\n",
    "top10 = sorted(avg_dir_rating.items(), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "names = [urllib.unquote(n).decode('utf-8') for (n,v) in top10]\n",
    "rating = [v for (n,v) in top10]\n",
    "acts = np.arange(len(names))\n",
    "acts = sorted(acts, reverse=True)\n",
    "\n",
    "plt.subplot2grid((2,3),(0,0), rowspan = 2)\n",
    "plt.barh(acts, rating, align='center', color='g')\n",
    "plt.yticks(acts, names)\n",
    "plt.xlim(0,10)\n",
    "plt.ylim(-1,len(names))\n",
    "plt.title(\"Top 10 directors who've done > 5 movies\", fontsize=16)\n",
    "plt.ylabel(\"Directors\", fontsize=14)\n",
    "plt.xlabel(\"Average rating\", fontsize=14)\n",
    "\n",
    "\n",
    "names = [urllib.unquote(n).decode('utf-8') for (n,v) in bottm10]\n",
    "rating = [v for (n,v) in bottm10]\n",
    "acts = np.arange(len(names))\n",
    "acts = sorted(acts, reverse=True)\n",
    "\n",
    "plt.subplot2grid((2,3),(0,1), rowspan = 2)\n",
    "plt.barh(acts, rating, align='center', color='r')\n",
    "plt.yticks(acts, names)\n",
    "plt.xlim(0,10)\n",
    "plt.ylim(-1,len(names))\n",
    "plt.title(\"Bottom 10 directors who've done > 5 movies\", fontsize=16)\n",
    "plt.xlabel(\"Average rating\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directors and their actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirs_acts = {}\n",
    "for d in all_dirs:\n",
    "    act_c = defaultdict(int)\n",
    "    for t, attr in films_dict.iteritems():\n",
    "        if d == attr['director']:\n",
    "            for a in attr['cast']:\n",
    "                act_c[a] += 1\n",
    "    dirs_acts[d] = act_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_top = ['Christopher Nolan', 'Quentin Tarantino', 'Stanley Kubrick']\n",
    "\n",
    "for ii in range(0,3):\n",
    "    vals = sorted(dirs_acts.get(names_top[ii]).items(), key=operator.itemgetter(1))\n",
    "\n",
    "    names = [n for (n,v) in vals if v > 1]\n",
    "    rating = [v for (n,v) in vals if v > 1]\n",
    "    \n",
    "    d = {}\n",
    "    for n in names:\n",
    "        d[n] = actor_scores.get(n)\n",
    "        \n",
    "    actor_r = [np.average(v) for n in names for (n1,v) in d.items() if n == n1]\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    acts = np.arange(len(actor_r))\n",
    "    acts = sorted(acts, reverse=True)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylim(0,11)\n",
    "    ax2.plot(acts,actor_r, 'ro')\n",
    "    ax2.set_ylabel('Cast IMDb Rating (avg. rating = %.2f)' % np.average(actor_r), color='r', fontsize=14)\n",
    "\n",
    "    acts = np.arange(len(names))\n",
    "    acts = sorted(acts, reverse=True)\n",
    "    ax1.plot(acts, rating, 'bo')\n",
    "    ax1.set_ylim(0,max(rating)+1)\n",
    "    ax1.set_xlim(-1,len(names))\n",
    "    ax1.set_xticks(acts)\n",
    "    ax1.set_xticklabels(names, rotation=90)\n",
    "    ax1.set_ylabel('# times in directors movies', color='b', fontsize=14)\n",
    "    ax1.grid()\n",
    "    plt.title(names_top[ii], fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_top = ['Brian Levant', 'Tyler Perry', 'Raja Gosnell']\n",
    "\n",
    "for ii in range(0,3):\n",
    "    vals = sorted(dirs_acts.get(names_top[ii]).items(), key=operator.itemgetter(1))\n",
    "\n",
    "    names = [n for (n,v) in vals if v > 1]\n",
    "    rating = [v for (n,v) in vals if v > 1]\n",
    "    \n",
    "    d = {}\n",
    "    for n in names:\n",
    "        d[n] = actor_scores.get(n)\n",
    "        \n",
    "    actor_r = [np.average(v) for n in names for (n1,v) in d.items() if n == n1]\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    acts = np.arange(len(actor_r))\n",
    "    acts = sorted(acts, reverse=True)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylim(0,11)\n",
    "    ax2.plot(acts,actor_r, 'ro')\n",
    "    ax2.set_ylabel('Cast IMDb Rating (avg. rating = %.2f)' % np.average(actor_r), color='r', fontsize=14)\n",
    "\n",
    "    acts = np.arange(len(names))\n",
    "    acts = sorted(acts, reverse=True)\n",
    "    ax1.plot(acts, rating, 'bo')\n",
    "    ax1.set_ylim(0,max(rating)+1)\n",
    "    ax1.set_xlim(-1,len(names))\n",
    "    ax1.set_xticks(acts)\n",
    "    ax1.set_xticklabels(names, rotation=90)\n",
    "    ax1.set_ylabel('# times in directors movies', color='b', fontsize=14)\n",
    "    plt.title(names_top[ii], fontsize=16)\n",
    "    ax1.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_r_act_r = {}\n",
    "for dirc in all_dirs:\n",
    "    vals = sorted(dirs_acts.get(dirc).items(), key=operator.itemgetter(1))\n",
    "\n",
    "    names = [n for (n,v) in vals if v > 1]\n",
    "    occurs = [v for (n,v) in vals if v > 1]\n",
    "\n",
    "    d = {}\n",
    "    for n in names:\n",
    "        d[n] = actor_scores.get(n)\n",
    "\n",
    "    for n in names:\n",
    "        for (n1,v) in d.items():\n",
    "            if n == n1:\n",
    "                dir_r_act_r[dirc] = {'dir_r': np.average(dir_rating.get(dirc)), 'act_r': np.average(v)}\n",
    "    \n",
    "#print len(dir_r_act_r.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate director rating vs. cast rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drs = []\n",
    "ars = []\n",
    "for k, v in dir_r_act_r.iteritems():\n",
    "    drs.append(v['dir_r'])\n",
    "    ars.append(v['act_r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(drs,ars)\n",
    "plt.xlim(0,10)\n",
    "plt.ylim(0,10)\n",
    "plt.title('Director rating vs. his cast rating (actors in > 1 movie)', fontsize=18)\n",
    "plt.ylabel('Actors IMDb Rating', fontsize=14)\n",
    "plt.xlabel('Directors IMDb Rating', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome**\n",
    "\n",
    "The outcome is quite a few plots and graphs which gives some insight into the dataset and maybe provides the user with some information they didn't have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion. Think critically about your creation\n",
    "### What went well?\n",
    "We managed to get a pretty good dataset and got some nice insights into its relationships. We may not have discovered some groundbreaking patterns in moviemaking, but we did perform the analysis as we wanted and got some information we didn't have before.\n",
    "\n",
    "### What is still missing? What could be improved?, Why?\n",
    "**Dataset**\n",
    "\n",
    "We found that the 'gross' specified in the Kaggle dataset is actually domestic gross and not worldwide gross. This does give our analysis a sort of skewed output. Also the budget could've been better formatted, so that all budgets are of the same currency, for example USD. \n",
    "\n",
    "**Sentiment**\n",
    "\n",
    "When calculating the sentiment for each actor we merely averaged the sentiment of all the movies the actor was in. It would be interesting if we could have the specific lines for each actor available so that the sentiment is calculated for each role instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
